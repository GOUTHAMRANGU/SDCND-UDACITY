{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOLUTION TO THE PROJECT TRAFFIC SIGN CLASSIFIER.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions to be answered in this project following the rubric.\n",
    "- 1)  Summary of the given dataset.\n",
    "- 2)  Vizualization of the given dataset.\n",
    "- 3)  What are the pre-porcessing techniques used and why?\n",
    "- 4)  Summarize the train and test data setup, data augmentation methods.\n",
    "- 5)  How does the final architecture of the setup look like, and why?. Give a visual feed.\n",
    "- 6)  HOW WAS THE MODEL TRAINED, which optimizer is used and why, what is the batch size, number of epochs, and hyperparameters.\n",
    "- 7)  What are the approaches followed towards a solution to the stated problem. How good is the final approach (based on the         percentage of accuracy).\n",
    "- 8)  Test the model on five new German trafffic sign images from web.\n",
    "- 9)  Is the model able to perform equally well on captured pictures or a live camera stream when compared to testing on the           dataset.\n",
    "- 10) Use the model's softmax probabilities to visualize the certainty of its predictions, tf.nn.top_k could prove helpful here.       Which predictions is the model certain of? Uncertain? If the model was incorrect in its initial prediction, does the             correct prediction appear in the top k? (k should be 5 at most)\n",
    "- 11) If necessary, provide documentation for how an interface was built for your model to load and classify newly-acquired           images.\n",
    "- 12) Discuss how you used the visual output of your trained network's feature maps to show that it had learned to look for           interesting characteristics in traffic sign images "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps followed sequentially during the exection of the classifier system:\n",
    "#### Step 1: Data Exploration.\n",
    "#### Step 2: Visualize the Data and Summarize the data.\n",
    "#### Step 3: Design and Test a model Architecture.\n",
    "#### Step 4: Test the model on new Images.\n",
    "#### Step 5: Store the Model for future use.\n",
    "#### Step 6: Summarize the entire solution.\n",
    "#### Step 7: Acknowledgements and sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : Data Exploration:\n",
    "First we need to familiarize ourselves with the dataset. This problem deals with the dataset of German Traffic Signs. During data exploration we need to see if the datasets are well balenced, because if they are no so, the model can get biased towards a class with higher number of images, which is to be avoided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import the required libraries and packages\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import csv\n",
    "import pickle\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.gridspec as gridspec #check step:7 #http://matplotlib.org/users/gridspec.html\n",
    "import os\n",
    "import sys\n",
    "#import time as time\n",
    "from timeit import default_timer as timer # check step:7 #https://docs.python.org/2/library/timeit.html\n",
    "import random\n",
    "from sklearn.preprocessing import OneHotEncoder # check step:7 #http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n",
    "#from sklearn.cross_validation import train_test_split, this is depricated in tensorflow 1.0 and used as .model_selection method.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image as pimag\n",
    "from IPython.display import Image as ipimag\n",
    "from IPython.display import display as ipdis\n",
    "import pandas as pd\n",
    "import prettytensor as pt # checkstep:7 # https://github.com/google/prettytensor\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix # check step:7 #http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "\n",
    "# var init\n",
    "indir = 'traffic_sign_data'\n",
    "outdir = 'modelsol'\n",
    "sess = tf.InteractiveSession() # check step:7 #https://www.tensorflow.org/versions/r0.11/api_docs/python/client/session_management#InteractiveSession\n",
    "SEED = 200\n",
    "random.seed(SEED) # http://stackoverflow.com/questions/22639587/random-seed-what-does-it-do\n",
    "np.random.seed(SEED)\n",
    "tf.set_random_seed(SEED) # https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  LOAD THE DATA\n",
    "def load_data(trainf,testf,signf): \n",
    "    training_file  = indir +'/'+trainf\n",
    "    testing_file   = indir +'/'+testf\n",
    "    classnames_file = indir +'/'+signf\n",
    "\n",
    "    classnames = []\n",
    "    with open(classnames_file) as _f:\n",
    "        rows = csv.reader(_f, delimiter=',')\n",
    "        next(rows, None)  # skip the headers\n",
    "        for i, row in enumerate(rows):\n",
    "            assert(i==int(row[0]))\n",
    "            classnames.append(row[1])\n",
    " \n",
    "    with open(training_file, mode='rb') as f:\n",
    "        train = pickle.load(f)\n",
    "    with open(testing_file, mode='rb') as f:\n",
    "        test = pickle.load(f)\n",
    "\n",
    "    X_train, y_train = train['features'], train['labels']\n",
    "    X_test, y_test   = test['features'], test['labels']\n",
    "    \n",
    "    \n",
    "    X_train  = X_train.astype(np.float32)\n",
    "    y_train  = y_train.astype(np.int32)\n",
    "    X_test   = X_test.astype(np.float32)\n",
    "    y_test   = y_test.astype(np.int32)\n",
    "    \n",
    "    return  classnames, X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def OHE_labels(Y_tr,N_classes):\n",
    "    OHC = OneHotEncoder()\n",
    "    \n",
    "    Y_ohc = OHC.fit(np.arange(N_classes).reshape(-1, 1))\n",
    "    Y_labels = Y_ohc.transform(Y_tr.reshape(-1, 1)).toarray()\n",
    "    return Y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_train = OHE_labels(y_train,43)\n",
    "labels_test = OHE_labels(y_test,43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_OHE(cls,y):\n",
    "    check = np.linalg.norm(np.argmax(cls,axis=1)-y)\n",
    "    if check == 0:\n",
    "        print('One hot encoding correct, ok to proceed')\n",
    "    else:\n",
    "        print('One hot encoding doesnt match the output, check code!!!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check_OHE(labels_test,y_test)\n",
    "check_OHE(labels_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#image normalization\n",
    "#image = image/255.-.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given: The pickled data is a dictionary with 4 key/value pairs:\n",
    "\n",
    "- `'features'` is a 4D array containing raw pixel data of the traffic sign images, (num examples, width, height, channels).\n",
    "- `'labels'` is a 1D array containing the label/class id of the traffic sign. The file `signnames.csv` contains id -> name mappings for each id.\n",
    "- `'sizes'` is a list containing tuples, (width, height) representing the original width and height the image.\n",
    "- `'coords'` is a list containing tuples, (x1, y1, x2, y2) representing coordinates of a bounding box around the sign in the image. **THESE COORDINATES ASSUME THE ORIGINAL IMAGE. THE PICKLED DATA CONTAINS RESIZED VERSIONS (32 by 32) OF THESE IMAGES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Getting dataset parameters.\n",
    "classnames, X_train, y_train, X_test, y_test = load_data(trainf='train.p',testf='test.p',signf='signnames.csv') \n",
    "# Save Original Test for future evaluation\n",
    "#X_test_orig = X_test\n",
    "#y_test_orig = y_test\n",
    " \n",
    "# TODO: Number of training examples \n",
    "num_train = len(X_train)\n",
    "\n",
    "# TODO: Number of testing examples.\n",
    "num_test = len(X_test)\n",
    "\n",
    "# TODO: What's the shape of an traffic sign image?\n",
    "_, height, width, channel = X_train.shape\n",
    "image_shape = (height, width, channel)\n",
    "\n",
    "# TODO: How many unique classes/labels there are in the dataset.\n",
    "num_class = len(np.unique(y_train))\n",
    "\n",
    "\n",
    "print(\"Number of training examples =\", num_train )\n",
    "print(\"Number of testing examples =\", num_test )\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", num_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we know that our dataset has 43 classes and train/test is approximately = 3, the image shape is 32x32x3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Visualize and summarize the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Q1) Visualize the given data.\n",
    "-  Q2) Summarize the given data\n",
    "- Answers to these questions are written in markdown cell after visualizing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sorting the given train data in ascending order with labels as keys.\n",
    "data_i = [[i,sum(y_train == i)] for i in range(len(np.unique(y_train)))]\n",
    "data_sorted = sorted(data_i, key=lambda x: x[1])\n",
    "sorted_data = [data_sorted[i][0] for i in range(len(data_sorted))]\n",
    "print(sorted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A helper function designed to plot the images.\n",
    "def insert_subimage(image, sub_image, y, x): \n",
    "    h, w, c = sub_image.shape\n",
    "    image[y:y+h, x:x+w, :]=sub_image \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualizing the train data.\n",
    "images, labels = X_train, y_train\n",
    "\n",
    "#results image, is the on over which rest sub images are drawn\n",
    "num_sample=10\n",
    "results_image = 255.*np.ones(shape=(num_class*height,(num_sample+4+22)*width, channel),dtype=np.float32)\n",
    "z=0\n",
    "for c in sorted_data:\n",
    "    #make mean\n",
    "    idx = list(np.where(labels== c)[0])\n",
    "    mean_image = np.average(images[idx], axis=0)\n",
    "    insert_subimage(results_image, mean_image, z*height, width)\n",
    "\n",
    "\n",
    "    #make random sample\n",
    "    for n in range(num_sample):\n",
    "        sample_image = images[np.random.choice(idx)]\n",
    "        insert_subimage(results_image, sample_image, z*height, (3+n)*width)\n",
    "\n",
    "    #print summary\n",
    "    count=len(idx)\n",
    "    percentage = float(count)/float(len(images))\n",
    "    cv2.putText(results_image, '%02d:%-6s'%(c, classnames[c]), ((4+num_sample)*width, int((z+.7)*height)),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,0),1)\n",
    "    cv2.putText(results_image, '[%4d]'%(count), ((2+num_sample+22)*width, int((z+0.7)*height)),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,255),1)\n",
    "    cv2.rectangle(results_image,((2+num_sample+16)*width, z*height),((2+num_sample+16)*width + round(percentage*3000), (z+1)*height),(255,0,10),-1)\n",
    "    z+=1\n",
    "\n",
    "# Save image to outdir\n",
    "cv2.imwrite(outdir+'/train_data_summary.jpg',cv2.cvtColor(results_image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (25,25)\n",
    "plt.imshow(results_image.astype(np.uint8))\n",
    "plt.axis('off') \n",
    "plt.show()\n",
    "\n",
    "print('Column [1]: Mean image for the data set corresponding to the respective image label')\n",
    "print('Column [2-11]: 10 Sample images')\n",
    "print('Column [12]: Sign label and Sign name')\n",
    "print('Column [13]: HISTOGRAM of entire train dataset')\n",
    "print('Column [14]: Number of images per class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Summary:\n",
    "The mean images look fine for every class, so it might not be too difficult to train the classifier. By looking at the sample images it is clear that brightness is varied largley across the data which should be handled. It is also clear from the histogram that the number of images per class are not equal, so data augmentation is necessary. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 : Design and Test a model Architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q3) What are the pre-porcessing techniques used and why?\n",
    "- Q4) Summarize the train and test data setup, data augmentation methods.\n",
    "- Q5) How does the final architecture of the setup look like, and why?. Give a visual feed.\n",
    "- Q6) HOW WAS THE MODEL TRAINED, which optimizer is used and why, what is the batch size, number of epochs, and hyperparameters.\n",
    "- Q7) What are the approaches followed towards a solution to the stated problem. How good is the final approach (based on the         percentage of accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answers to these questions are written along the proceess of design and testing the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data pre-processing\n",
    "Some of the ideas were inspired from the detail from https://arxiv.org/abs/1606.02228 . This paper gave a good understanding and description about the architecture and hyperparameters for a network.This paper also gives a verygood overview of CNN architectures https://arxiv.org/pdf/1511.02992.pdf .\n",
    "- The image is not converted to grayscale as COLOR is one important feature for traffic signs along with geometrics.\n",
    "- Histogram equilization on the input image didnot bring improvement in training accuracy but lead to loss during validation.\n",
    "- Initial normalization and stadardization did hurt the performance as well.\n",
    "###### ** So, no pre-processing is done on the entire dataset at once, but only create new data that is like the transformed version of current for data augmentation. The data augmentation is done by transforming the images, transformations are performed to change the orientation, to shift, to shear and to enhace the image in terms of its brightness, contrast and saturation.**\n",
    "- The above mentioned methods are only applied with a certain range, for example the image is rotated randomly only in the         range of roughly  +- 40. The idea of image flipping is also considered to be a good data augmentation technique(but all   the classes donot obey a single mirror plane, as the classes are based on geometrics of the image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We work on the copy of dataset rather than on dataset itself. o_ represents original copy.\n",
    "Xo_train, yo_train, Xo_test, yo_test = X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combine train and test into one dataset and split it later on\n",
    "\n",
    "X = np.concatenate((X_train, X_test), axis=0)\n",
    "y = np.concatenate((y_train, y_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a function to show the class distributions when needed\n",
    "def show_classes_distribution(labels, title = None):\n",
    "    # Count classes samples\n",
    "    uniq_labels = sorted(set(labels.tolist()))\n",
    "    #print(uniq_labels)\n",
    "    n_labels = len(uniq_labels)\n",
    "    class_counts = np.zeros([n_labels])\n",
    "    for c in uniq_labels:\n",
    "        class_counts[c] = np.sum(labels == c)\n",
    "    y_pos = np.arange(n_labels)\n",
    "    plt.figure(figsize=(18, 5))\n",
    "    plt.bar(uniq_labels, class_counts)\n",
    "    plt.xticks(y_pos)\n",
    "    plt.ylabel('Classes Counts')\n",
    "    if title: plt.title(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This part of code is borrowed from http://navoshta.com/traffic-signs-classification/\n",
    "# Flipping the images to gain more train data.\n",
    "def extend_data_by_flipping(images, labels):\n",
    "\n",
    "    X=images\n",
    "    y=labels\n",
    "\n",
    "    # Classes of signs that, when flipped horizontally, should still be classified as the same class\n",
    "    self_flippable_horizontally = np.array([11, 12, 13, 15, 17, 18, 22, 26, 30, 35])\n",
    "    # Classes of signs that, when flipped vertically, should still be classified as the same class\n",
    "    self_flippable_vertically = np.array([1, 5, 12, 15, 17])\n",
    "    # Classes of signs that, when flipped horizontally and then vertically, should still be classified as the same class\n",
    "    self_flippable_both = np.array([32, 40])\n",
    "    # Classes of signs that, when flipped horizontally, would still be meaningful, but should be classified as some other class\n",
    "    cross_flippable = np.array([\n",
    "        [19, 20],\n",
    "        [33, 34],\n",
    "        [36, 37],\n",
    "        [38, 39],\n",
    "        [20, 19],\n",
    "        [34, 33],\n",
    "        [37, 36],\n",
    "        [39, 38],\n",
    "    ])\n",
    "    num_classes = 43\n",
    "\n",
    "    X_extended = np.empty([0, X.shape[1], X.shape[2], X.shape[3]], dtype=np.float32)\n",
    "    y_extended = np.empty([0], dtype=np.int32)\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        # First copy existing data for this class\n",
    "        X_extended = np.append(X_extended, X[y == c], axis=0)\n",
    "        # If we can flip images of this class horizontally and they would still belong to said class...\n",
    "        if c in self_flippable_horizontally:\n",
    "            # ...Copy their flipped versions into extended array.\n",
    "            X_extended = np.append(X_extended, X[y == c][:, :, ::-1, :], axis=0)\n",
    "        # If we can flip images of this class horizontally and they would belong to other class...\n",
    "        if c in cross_flippable[:, 0]:\n",
    "            # ...Copy flipped images of that other class to the extended array.\n",
    "            flip_class = cross_flippable[cross_flippable[:, 0] == c][0][1]\n",
    "            X_extended = np.append(X_extended, X[y == flip_class][:, :, ::-1, :], axis=0)\n",
    "        # Fill labels for added images set to current class.\n",
    "        y_extended = np.append(y_extended, np.full((X_extended.shape[0] - y_extended.shape[0]), c, dtype=np.int32))\n",
    "\n",
    "        # If we can flip images of this class vertically and they would still belong to said class...\n",
    "        if c in self_flippable_vertically:\n",
    "            # ...Copy their flipped versions into extended array.\n",
    "            X_extended = np.append(X_extended, X_extended[y_extended == c][:, ::-1, :, :], axis=0)\n",
    "        # Fill labels for added images set to current class.\n",
    "        y_extended = np.append(y_extended, np.full((X_extended.shape[0] - y_extended.shape[0]), c, dtype=np.int32))\n",
    "\n",
    "        # If we can flip images of this class horizontally AND vertically and they would still belong to said class...\n",
    "        if c in self_flippable_both:\n",
    "            # ...Copy their flipped versions into extended array.\n",
    "            X_extended = np.append(X_extended, X_extended[y_extended == c][:, ::-1, ::-1, :], axis=0)\n",
    "        # Fill labels for added images set to current class.\n",
    "        y_extended = np.append(y_extended, np.full((X_extended.shape[0] - y_extended.shape[0]), c, dtype=np.int32))\n",
    "\n",
    "    extend_datas  = X_extended\n",
    "    extend_labels = y_extended\n",
    "    return (extend_datas, extend_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#flip the data\n",
    "train_images, train_labels = extend_data_by_flipping(X, y)\n",
    "num_train_flip = len(train_images)\n",
    "print(num_train_flip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perturb(image, keep, angle_limit=15, scale_limit=0.1, translate_limit=3, distort_limit=3, illumin_limit=0.7):\n",
    "\n",
    "    u=np.random.uniform()\n",
    "    if u>keep :\n",
    "        #geometric -------------\n",
    "        (W, H, C) = image.shape\n",
    "        center = np.array([W / 2., H / 2.])\n",
    "        da = np.random.uniform(low=-1, high=1) * angle_limit/180. * math.pi\n",
    "        scale = np.random.uniform(low=-1, high=1) * scale_limit + 1\n",
    "\n",
    "        cc = scale*math.cos(da)\n",
    "        ss = scale*math.sin(da)\n",
    "        rotation    = np.array([[cc, ss],[-ss,cc]])\n",
    "        translation = np.random.uniform(low=-1, high=1, size=(1,2)) * translate_limit\n",
    "        distort     = np.random.standard_normal(size=(4,2)) * distort_limit\n",
    "\n",
    "        pts1 = np.array([[0., 0.], [0., H], [W, H], [W, 0.]])\n",
    "        pts2 = np.matmul(pts1-center, rotation) + center  + translation\n",
    "\n",
    "        #add perspective noise\n",
    "        pts2 = pts2 + distort\n",
    "\n",
    "\n",
    "        #http://milindapro.blogspot.jp/2015/05/opencv-filters-copymakeborder.html\n",
    "        matrix  = cv2.getPerspectiveTransform(pts1.astype(np.float32), pts2.astype(np.float32)) \n",
    "        perturb = cv2.warpPerspective(image, matrix, (W, H), flags=cv2.INTER_LINEAR,\n",
    "                                      borderMode=cv2.BORDER_REFLECT_101)  # BORDER_WRAP  #BORDER_REFLECT_101  #cv2.BORDER_CONSTANT  BORDER_REPLICATE\n",
    "\n",
    "        #illumination -------------\n",
    "        #from mxnet code\n",
    "        \n",
    "        #brightness\n",
    "        alpha = 1.0 + illumin_limit*random.uniform(-1, 1)\n",
    "        perturb *= alpha\n",
    "        perturb = np.clip(perturb,0.,255.)\n",
    "        pass\n",
    "\n",
    "        #contrast\n",
    "        coef = np.array([[[0.299, 0.587, 0.114]]]) #rgb to gray (YCbCr) :  Y = 0.299R + 0.587G + 0.114B\n",
    "\n",
    "        alpha = 1.0 + illumin_limit*random.uniform(-1, 1)\n",
    "        gray = perturb * coef\n",
    "        gray = (3.0 * (1.0 - alpha) / gray.size) * np.sum(gray)\n",
    "        perturb *= alpha\n",
    "        perturb += gray\n",
    "        perturb = np.clip(perturb,0.,255.)\n",
    "        pass\n",
    "\n",
    "        #saturation\n",
    "        coef = np.array([[[0.299, 0.587, 0.114]]]) #rgb to gray (YCbCr) :  Y = 0.299R + 0.587G + 0.114B\n",
    "\n",
    "        alpha = 1.0 + illumin_limit*random.uniform(-1, 1)\n",
    "        gray = perturb * coef\n",
    "        gray = np.sum(gray, axis=2, keepdims=True)\n",
    "        gray *= (1.0 - alpha)\n",
    "        perturb *= alpha\n",
    "        perturb += gray\n",
    "        perturb = np.clip(perturb,0.,255.)\n",
    "        pass\n",
    "\n",
    "        return perturb\n",
    "\n",
    "    else:\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import ndimage\n",
    "def transform_image(image,ang_range=30,shear_range=0.2,trans_range=3):\n",
    "    (W, H, C) = image.shape\n",
    "    pts1 = np.array([[0., 0.], [0., H], [W, H], [W, 0.]])\n",
    "\n",
    "    # Rotation\n",
    "\n",
    "    ang_rot = np.random.uniform(low= -ang_range,high=ang_range)\n",
    "    rows,cols,ch = image.shape    \n",
    "    Rot_M = cv2.getRotationMatrix2D((cols/2,rows/2),ang_rot,1)\n",
    "\n",
    "    # Translation\n",
    "    tr_x = trans_range*np.random.uniform()-trans_range/2\n",
    "    tr_y = trans_range*np.random.uniform()-trans_range/2\n",
    "    Trans_M = np.float32([[1,0,tr_x],[0,1,tr_y]])\n",
    "\n",
    "    # Shear\n",
    "    pts1 = np.float32([[5,5],[20,5],[5,20]])\n",
    "\n",
    "    pt1 = 5+shear_range*np.random.uniform()-shear_range/2\n",
    "    pt2 = 20+shear_range*np.random.uniform()-shear_range/2\n",
    "\n",
    "    pts2 = np.float32([[pt1,5],[pt2,pt1],[5,pt2]])\n",
    "\n",
    "    shear_M = cv2.getAffineTransform(pts1,pts2)\n",
    "        \n",
    "    image = cv2.warpAffine(image,Rot_M,(cols,rows))\n",
    "    image = cv2.warpAffine(image,Trans_M,(cols,rows))\n",
    "    image = cv2.warpAffine(image,shear_M,(cols,rows))\n",
    "    IM = image\n",
    "    return IM\n",
    "def random_gaussian_filter(img):\n",
    "    sigma = np.random.choice([0.2,0.5,0.7,1])\n",
    "    return ndimage.gaussian_filter(img, sigma=sigma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_images(X, y):\n",
    "    # Count classes samples\n",
    "    uniq_labels = sorted(set(y.tolist()))\n",
    "    n_labels = len(uniq_labels)\n",
    "    class_counts = np.zeros([n_labels])\n",
    "    \n",
    "    all_ind = np.arange(len(y))\n",
    "    img_indices = {}\n",
    "    for c in uniq_labels:\n",
    "        class_counts[c] = np.sum(y == c)\n",
    "        img_indices[c] = all_ind[y == c]\n",
    "    \n",
    "    print(\"len_X =\", len(X))\n",
    "    print(\"len_y =\", len(y))\n",
    "    print(\"class_counts =\", class_counts)\n",
    "        \n",
    "    max_num = np.max(class_counts)\n",
    "    min_num = np.min(class_counts)\n",
    "    print(\"max_num =\", max_num)\n",
    "    print(\"min_num =\", min_num)\n",
    "\n",
    "    errors =0\n",
    "    for c in range(n_labels):\n",
    "        c_num = np.sum(y == c)\n",
    "        num = max_num - c_num\n",
    "        nimgs = []\n",
    "        nimgs_labels = []\n",
    "        \n",
    "        num_range = range(int(num)) # tqdm progress\n",
    "        \n",
    "        for n in num_range:\n",
    "            sample = np.random.choice(img_indices[c])\n",
    "            try:\n",
    "                nimg = perturb(X[sample],keep=0)\n",
    "            except:\n",
    "                errors += 1\n",
    "                nimg = random_gaussian_filter(X[sample])\n",
    "            nimgs.append(nimg)\n",
    "            nimgs_labels.append(c)\n",
    "            \n",
    "        if len(nimgs) > 0:\n",
    "            X = np.append(X, np.array(nimgs), axis=0)\n",
    "            y = np.append(y, nimgs_labels)\n",
    "            class_counts[c] += num\n",
    "\n",
    "    print(\"errors =\", errors)\n",
    "    return X, y\n",
    "    \n",
    "X1, y1 = generate_images(X, y)\n",
    "show_classes_distribution(y1, 'All Dataset')\n",
    "\n",
    "\n",
    "\n",
    "# Split Data into Train/Validation/Test\n",
    "def train_validation_test(X, y):\n",
    "    # Shuffle it\n",
    "    r_idx = np.random.permutation(len(X))\n",
    "    print(\"len_X =\", len(X))\n",
    "    print(\"len_y =\", len(y))\n",
    "    X = X[r_idx]\n",
    "    y = y[r_idx]\n",
    "\n",
    "    # Train/Valid/Test distributions\n",
    "    p_train = 0.8\n",
    "    p_valid = 0.1\n",
    "    p_test = 0.1\n",
    "\n",
    "    # Sanity check\n",
    "    assert p_train + p_valid + p_test == 1.0\n",
    "\n",
    "    N = len(X)\n",
    "    train_idx = int(N*p_train)\n",
    "    valid_idx = int(N*(p_train+p_valid))\n",
    "    X_train = X[:train_idx]\n",
    "    y_train = y[:train_idx]\n",
    "    X_valid = X[train_idx:valid_idx]\n",
    "    y_valid = y[train_idx:valid_idx]\n",
    "    X_test = X[valid_idx:]\n",
    "    y_test = y[valid_idx:]\n",
    "                \n",
    "    assert len(X_train) + len(X_valid) + len(X_test) == N\n",
    "    assert len(y_train) + len(y_valid) + len(y_test) == N\n",
    "\n",
    "    \n",
    "    return X_train, y_train, X_valid, y_valid, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_valid, y_valid, X_test, y_test = train_validation_test(X1, y1)\n",
    "\n",
    "\n",
    "show_classes_distribution(y_train, 'Train + Generated')\n",
    "show_classes_distribution(y_test, 'Validation + Generated')\n",
    "show_classes_distribution(y_test, 'Test + Generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''try:\n",
    "    with open(X_train.pickle, 'wb') as f:\n",
    "        pickle.dump(X_train, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', X_train.pickle, ':', e)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''#number of images per class\n",
    "uniq_labels = sorted(set(y_train.tolist()))\n",
    "n_labels = len(uniq_labels)\n",
    "clcount_train = np.zeros([n_labels])\n",
    "clcount_test = np.zeros([n_labels])\n",
    "for c in uniq_labels:\n",
    "    clcount_train[c] = np.sum(y_train == c)\n",
    "    clcount_test[c] = np.sum(y_test == c)\n",
    "print('train:',clcount_train,'test:',clcount_test)'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "images, labels = X_train, y_train\n",
    "imageso, labelso = Xo_train, yo_train\n",
    "num_train_flip = len(Xo_train)\n",
    "num_sample = 20\n",
    "perturbance_per_sample = 20\n",
    "\n",
    "results_image = 255. * np.ones(shape=(num_sample * height, (perturbance_per_sample+1)* width+2, channel),dtype=np.float32)\n",
    "\n",
    "for j in range(num_sample):\n",
    "    i = random.randint(0, num_train_flip - 1)\n",
    "    idx = list(np.where(labels== j)[0])\n",
    "    idxo = list(np.where(labelso==j)[0])\n",
    "\n",
    "    image = imageso[np.random.choice(idxo)]\n",
    "    insert_subimage(results_image, image, j * height, 0)\n",
    "\n",
    "    for k in range(0, perturbance_per_sample):\n",
    "        t_image = images[np.random.choice(idx)]\n",
    "        insert_subimage(results_image, t_image, j*height, (k+1)*width+2)\n",
    "\n",
    "         \n",
    "cv2.imwrite(outdir+'/data_transform.jpg',cv2.cvtColor(results_image, cv2.COLOR_BGR2RGB))\n",
    "plt.rcParams[\"figure.figsize\"] = (25,25)\n",
    "plt.imshow(results_image.astype(np.uint8))\n",
    "plt.axis('off') \n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_orig = X_train\n",
    "X_test_orig = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = (X_train - X_train.mean()) / (np.max(X_train) - np.min(X_train))\n",
    "X_test = (X_test - X_test.mean()) / (np.max(X_test) - np.min(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_norm_image(image_index):\n",
    "    \"\"\"Plots original image on the left and normalised image on the right.\"\"\"\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.imshow(X_train_orig[image_index])\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.imshow(X_train[image_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_norm_image(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network parameters\n",
    "n_input = 32 * 32 * 3\n",
    "nb_filters = 32\n",
    "kernel_size = (3, 3)\n",
    "input_shape = (32, 32, 3)\n",
    "n_fc1 = 512\n",
    "n_fc2 = 128\n",
    "in_channels = 3\n",
    "pool_size = 2 # i.e. (2,2)\n",
    "\n",
    "dropout_conv = 0.9\n",
    "dropout_fc = 0.9\n",
    "\n",
    "weights_stddev = 0.1\n",
    "weights_mean = 0.0\n",
    "biases_mean = 0.0\n",
    "\n",
    "padding = 'VALID'\n",
    "if padding == 'SAME':\n",
    "    conv_output_length = 6\n",
    "elif padding == 'VALID':\n",
    "    conv_output_length = 5\n",
    "else:\n",
    "    raiseException(\"Unknown padding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "x_unflattened = tf.placeholder(\"float\", [None, 32, 32, 3])\n",
    "x = x_unflattened\n",
    "\n",
    "y_rawlabels = tf.placeholder(\"int32\", [None])\n",
    "y = tf.one_hot(y_rawlabels, depth=43, on_value=1., off_value=0., axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Create model\n",
    "\n",
    "def conv2d(x, W, b, strides=3):\n",
    "    \"\"\"Conv2D wrapper, with bias and relu activation\"\"\"\n",
    "    # strides = [batch, in_height, in_width, channels]\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, k=2, padding_setting='SAME'):\n",
    "    \"\"\"MaxPool2D wrapper.\"\"\"\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding=padding_setting)\n",
    "\n",
    "def conv_net(model_x, model_weights, model_biases, model_pool_size, \n",
    "             model_dropout_conv, model_dropout_fc, padding='SAME'):\n",
    "    \"\"\"Convolutional neural network model.\"\"\"\n",
    "    # Convolution Layer 1\n",
    "    conv1 = conv2d(model_x, model_weights['conv1'], model_biases['conv1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=model_pool_size, padding_setting=padding)\n",
    "    conv1 = tf.nn.dropout(conv1, model_dropout_conv)\n",
    "\n",
    "    # Fully connected layer 1\n",
    "    # Reshape conv1 output to fit fully connected layer input\n",
    "    conv1_shape = conv1.get_shape().as_list()\n",
    "    fc1 = tf.reshape(conv1, [-1, conv1_shape[1]*conv1_shape[2]*conv1_shape[3]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, model_weights['fc1']), model_biases['fc1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, model_dropout_fc)\n",
    "    # Fully connected layer 2\n",
    "    fc2 = tf.add(tf.matmul(fc1, model_weights['fc2']), model_biases['fc2'])\n",
    "    fc2 = tf.nn.relu(fc2)\n",
    "    fc2 = tf.nn.dropout(fc2, model_dropout_fc)\n",
    "    # Output layer\n",
    "    output = tf.add(tf.matmul(fc2, model_weights['out']), model_biases['out'])\n",
    "    # Note: Softmax is outside the model\n",
    "    return output\n",
    "\n",
    "\n",
    "## Store layers weight & bias\n",
    "\n",
    "# NEW: initialise neurons with slightly positive initial bias\n",
    "# to avoid dead neurons.\n",
    "def weight_variable(shape, weight_mean, weight_stddev):\n",
    "    initial = tf.truncated_normal(shape, stddev=weight_stddev, mean=weight_mean)\n",
    "    # alt: tf.random_normal(shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape, bias_mean):\n",
    "    initial = tf.constant(bias_mean, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "weights = {\n",
    "    'conv1': weight_variable([kernel_size[0], kernel_size[1], in_channels, nb_filters], weights_mean, weights_stddev),\n",
    "    'fc1': weight_variable([nb_filters * conv_output_length**2, n_fc1], weights_mean, weights_stddev),\n",
    "    'fc2': weight_variable([n_fc1, n_fc2], weights_mean, weights_stddev),\n",
    "    'out': weight_variable([n_fc2, n_classes], weights_mean, weights_stddev)\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'conv1': bias_variable([nb_filters], biases_mean),\n",
    "    'fc1': bias_variable([n_fc1], biases_mean),\n",
    "    'fc2': bias_variable([n_fc2], biases_mean),\n",
    "    'out': bias_variable([n_classes], biases_mean)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "learning_rate = 0.001\n",
    "initial_learning_rate = learning_rate\n",
    "training_epochs = 150\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "n_train = len(X_train)\n",
    "\n",
    "anneal_mod_frequency = 15\n",
    "# Annealing rate of 1: learning rate remains constant.\n",
    "annealing_rate = 1\n",
    "\n",
    "print_accuracy_mod_frequency = 1\n",
    "\n",
    "# Construct model\n",
    "pred = conv_net(x, weights, biases, pool_size, dropout_conv, dropout_fc, padding=padding)\n",
    "pred_probs = tf.nn.softmax(pred)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred,labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Function to initialise the variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "### RUN MODEL ###\n",
    "# Launch the graph\n",
    "sess = tf.Session()\n",
    "\n",
    "# Initialise variables\n",
    "sess.run(init)\n",
    "\n",
    "# Initialise time logs\n",
    "init_time = time.time()\n",
    "epoch_time = init_time\n",
    "\n",
    "five_epoch_moving_average = 0.\n",
    "epoch_accuracies = []\n",
    "\n",
    "# Training cycle\n",
    "for epoch in range(training_epochs):\n",
    "    if five_epoch_moving_average > 0.96:\n",
    "        break\n",
    "        \n",
    "    avg_cost = 0.\n",
    "\n",
    "    total_batch = int(n_train / batch_size)\n",
    "    # Loop over all batches\n",
    "    for i in range(total_batch):\n",
    "        batch_x, batch_y = np.array(X_train[i * batch_size:(i + 1) * batch_size]), \\\n",
    "                           np.array(y_train[i * batch_size:(i + 1) * batch_size])\n",
    "        # tf.train.batch([X_train, y_train], batch_size=100, enqueue_many=True)\n",
    "        # Run optimization op (backprop) and cost op (to get loss value)\n",
    "        _, c = sess.run([optimizer, cost], feed_dict={x_unflattened: batch_x, y_rawlabels: batch_y})\n",
    "        # Compute average loss\n",
    "        avg_cost += c / total_batch\n",
    "        # print(avg_cost)\n",
    "    # Display logs per epoch step\n",
    "    if epoch % display_step == 0:\n",
    "        print(\"Epoch:\", '%04d' % (epoch + 1), \"cost=\",\n",
    "              \"{:.9f}\".format(avg_cost))\n",
    "        last_epoch_time = epoch_time\n",
    "        epoch_time = time.time()\n",
    "        # print(\"Time since last epoch: \", epoch_time - last_epoch_time)\n",
    "    # Anneal learning rate\n",
    "    if (epoch + 1) % anneal_mod_frequency == 0:\n",
    "        learning_rate *= annealing_rate\n",
    "        print(\"New learning rate: \", learning_rate)\n",
    "\n",
    "    if (epoch + 1) % print_accuracy_mod_frequency == 0:\n",
    "        correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        # Line below needed only when not using `with tf.Session() as sess`\n",
    "        with sess.as_default():\n",
    "            epoch_accuracy = accuracy.eval({x_unflattened: X_val, y_rawlabels: y_val})\n",
    "            # TODO: optimise five_epoch_moving_average, e.g. using a queue            \n",
    "            epoch_accuracies.append(epoch_accuracy)\n",
    "            if epoch >= 4:\n",
    "                five_epoch_moving_average = np.sum(epoch_accuracies[epoch-5:epoch]) / 5\n",
    "                print(\"Five epoch moving average: \", five_epoch_moving_average)\n",
    "            print(\"Accuracy (validation):\", epoch_accuracy)\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "# Test model\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "# Calculate accuracy\n",
    "# accuracy_train = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "# print(\"Accuracy (train):\", accuracy_train.eval({x_unflattened: X_train, y_rawlabels: y_train}))\n",
    "train_predict_time = time.time()\n",
    "# print(\"Time to calculate accuracy on training set: \", train_predict_time - epoch_time)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "# Line below needed only when not using `with tf.Session() as sess`\n",
    "with sess.as_default():\n",
    "    print(\"Accuracy (test):\", accuracy.eval({x_unflattened: X_test, y_rawlabels: y_test}))\n",
    "test_predict_time = time.time()\n",
    "print(\"Time to calculate accuracy on test set: \", test_predict_time - train_predict_time)\n",
    "\n",
    "# Print parameters for reference\n",
    "print(\"\\nParameters:\")\n",
    "print(\"Learning rate (initial): \", initial_learning_rate)\n",
    "print(\"Anneal learning rate every \", anneal_mod_frequency, \" epochs by \", 1 - annealing_rate)\n",
    "print(\"Learning rate (final): \", learning_rate)\n",
    "print(\"Training epochs: \", training_epochs)\n",
    "print(\"Batch size: \", batch_size)\n",
    "print(\"Dropout (conv): \", dropout_conv)\n",
    "print(\"Dropout (fc): \", dropout_fc)\n",
    "print(\"Padding: \", padding)\n",
    "print(\"weights_mean: \", weights_mean)\n",
    "print(\"weights_stddev: \", weights_stddev)\n",
    "print(\"biases_mean: \", biases_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LAST BLOCK\n",
    "CADENCE IMPLEMENTATION: https://ip.cadence.com/uploads/901/TIP_WP_cnn_FINAL-pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:sdgpu]",
   "language": "python",
   "name": "conda-env-sdgpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
